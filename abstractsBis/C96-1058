Alter presenting a novel O(n 3) parsing algorithm
for dependency grammar, we develop
three contrasting ways to stochasticize
it. We propose (a) a lexical affinity mode]
where words struggle to modify each other,
(b) a sense tagging model where words fluctuate
randomly in their selectional preferences,
and (c) a generative model where
the speaker fleshes out each word's syntactic
and conceptual structure without regard to
the implications for the hearer. We also give
preliminary empirical results from evaluating
the three models' parsing performance
on annotated Wall Street Journal trMning
text (derived from the Penn Treebank). in
these results, the generative model performs
significantly better than the others, and
does about equally well at assigning part-of-speech
tags.